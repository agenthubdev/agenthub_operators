import requests
import io

from bs4 import BeautifulSoup
import tabula
from PyPDF2 import PdfReader
import pandas as pd

from ai_context import AiContext
from .base_operator import BaseOperator

class IngestData(BaseOperator):
    @staticmethod
    def declare_name():
        return 'Ingest Data'
    
    @staticmethod
    def declare_category():
        return BaseOperator.OperatorCategory.CONSUME_DATA.value

    @staticmethod    
    def declare_parameters():
        return [
            {
                "name": "data_uri",
                "data_type": "string",
                "placeholder": "Enter the URL to browse"
            },
            {
                "name": "uploaded_file_name",
                "data_type": "string",
                "placeholder": "Enter the name.pdf of the uploaded file"
            }
        ]
    
    @staticmethod    
    def declare_inputs():
        return [
            {
                "name": "data_uri",
                "data_type": "string",
                "optional": "1"
            },
            {
                "name": "uploaded_file_name",
                "data_type": "string",
                "optional": "1"
            }
        ]
    
    @staticmethod    
    def declare_outputs():
        return [
            {
                "name": "data",
                "data_type": "string",
            }
        ]

    def run_step(self, step, ai_context: AiContext):
        params = step['parameters']
        data_uri = params.get('data_uri') or ai_context.get_input('data_uri', self)
        param_file_name = params.get('uploaded_file_name')
        input_file_name = ai_context.get_input('uploaded_file_name', self)

        # If file name is provided via input we assume it was generated by some other operator this run.
        generated_this_run = False if param_file_name else True
        uploaded_file_name = param_file_name or input_file_name

        self.ingest(data_uri, uploaded_file_name, generated_this_run, ai_context)

    def ingest(self, data_uri, uploaded_file_name, generated_this_run, ai_context):
        if uploaded_file_name:
            ai_context.add_to_log(f"Loading {uploaded_file_name} from storage.")
            file_data = self.load_file_from_storage(uploaded_file_name, generated_this_run, ai_context)
            text = self.read_file(file_data, uploaded_file_name)
            ai_context.add_to_log(f"Content from uploaded file {uploaded_file_name} has been scraped.")
            ai_context.set_output('data', text, self)
        elif data_uri and self.is_url(data_uri):
            if data_uri.lower().endswith(".pdf"):
                file_data = self.load_pdf_from_uri(data_uri)
                text = self.read_pdf(file_data)
                ai_context.add_to_log(f"Content from PDF at {data_uri} has been scraped.")
            else:
                text = self.scrape_text(data_uri)
                ai_context.storage['ingested_url'] = data_uri
                ai_context.add_to_log(f"Content from {data_uri} has been scraped.")
            ai_context.set_output('data', text, self)
        else:
            ai_context.set_output('data', '', self)
            ai_context.add_to_log("No file or URL to read.")

    def load_file_from_storage(self, file_name, generated_this_run, ai_context):
        #If the file is generated this run we store it in the /run_id directory.
        if generated_this_run:
            file_data = ai_context.get_file(file_name, ai_context.get_run_id())
        else:
            file_data = ai_context.get_file(file_name)
        return file_data

    def read_file(self, file_data, file_name):
        if file_name.lower().endswith('.pdf'):
            return self.read_pdf(file_data)
        elif file_name.lower().endswith('.json'):
            return pd.read_json(io.StringIO(file_data.decode())).to_string()
        elif file_name.lower().endswith('.csv'):
            return pd.read_csv(io.StringIO(file_data.decode())).to_string()
        else:
            raise ValueError(f"Unsupported file format: {file_name}")

    def is_url(self, data_uri):
        #TODO: Add a real is_url check into utils.py
        return True

    def scrape_text(self, url):
        response = requests.get(url)
        bs = BeautifulSoup(response.text, "html.parser")

        for script in bs(["script", "style"]):
            script.extract()

        text = bs.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = "\n".join(chunk for chunk in chunks if chunk)

        return text
    
    def load_pdf_from_uri(self, url):
        response = requests.get(url)
        response.raise_for_status()  # Ensure we got a valid response
        return response.content
    
    def read_pdf(self, pdf):
        pd.set_option('display.max_colwidth', None)
        pdf_content = io.BytesIO(pdf)
        df_list = tabula.read_pdf(pdf_content, pages='all')
        
        # If tabula returned empty DataFrames, fall back to PyPDF2
        if all(df.empty for df in df_list):
            pdf_reader = PdfReader(pdf_content)
            text = []
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text.append(page.extract_text())
            return "\n".join(text)
        
        pdf_content = "\n".join(df.to_string(index=False) for df in df_list)
        return pdf_content